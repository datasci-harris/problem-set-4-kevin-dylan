---
title: "PS4"
Auther: Dizhe Xia
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

## Style Points (10 pts)

Have refered to the minilesson on code style

## Submission Steps (10 pts)

• Partner 1 (name and cnet ID): Qiyin Yao; qiyin
• Partner 2 (name and cnet ID):Dizhe Xia; dizhexia

“This submission is our work alone and complies with the 30538 integrity policy.” Add
your initials to indicate your agreement: **_QY_** **_DX_**

Late coins used this pset: **_1_** Late coins left after submission: **_2_**

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
I chose variables: 1. Provider Category Code: PRVDR_CTGRY_CD； 2. Provider Category Subtype Code: PRVDR_CTGRY_SBTYP_CD; 3. Facility Name: FAC_NAME; 4. Termination Code：PGM_TRMNTN_CD; 5. Address: City：CITY_NAME; 6. State Abbreviation：STATE_CD; 7. Certification Date：CRTFCTN_DT.

2. 
a.
```{python}
import pandas as pd

#import the data of pos2016
pos2016 = pd.read_csv("/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2016.csv")

#filter short term hospitals
short_term_hospitals = pos2016[(pos2016['PRVDR_CTGRY_CD'] == 1.0) & (pos2016['PRVDR_CTGRY_SBTYP_CD'] == 1.0)]

#print number of short term hospitals
num_short_term_hospitals = short_term_hospitals.shape[0]
print(f"Number of short term hospital: {num_short_term_hospitals}")
```

b.
The number of short term hospital is 7,245. I do not think the number is sensible. As the article mentioned that "There are nearly 5,000 short-term, acute care hospitals in the United States". Although the article was publised at mid 2016. But I do not think it is sensible to have such a huge increasing in short-term.

3. 
```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Combine file paths for each year
file_paths = {
    2016: "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2016.csv",
    2017: "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2017.csv",
    2018: "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2018.csv",
    2019: "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2019.csv"
}

# Dictionary to store the count of observations by year
observations_by_year = {}

# Loop through each year and file path
for year, path in file_paths.items():
    
    data = pd.read_csv(path, encoding='ISO-8859-1')
    
    short_term_hospitals = data[(data['PRVDR_CTGRY_CD'] == 1.0) & (data['PRVDR_CTGRY_SBTYP_CD'] == 1.0)]
    
    observations_by_year[year] = short_term_hospitals.shape[0]
    
    # Print the number of short-term hospitals for each year
    print(f"Year {year}: {observations_by_year[year]} short-term hospitals")

# Convert to a pandas Series for easy plotting
observations_series = pd.Series(observations_by_year)

# Plotting
plt.figure(figsize=(8, 6))
observations_series.plot(kind='bar', color='orange')
plt.ylim(6950, 8000)  
plt.xticks(rotation=45)
plt.xlabel('Year')
plt.ylabel('Number of Observations')
plt.title('Number of Short-Term Hospital Observations by Year')
plt.show()
```

4. 
a.
```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Define file paths for each year
file_paths = {
    2016: "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2016.csv",
    2017: "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2017.csv",
    2018: "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2018.csv",
    2019: "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2019.csv"
}

# Dictionary to store unique hospital counts by year
unique_hospitals_by_year = {}

# Loop through each file to calculate the unique hospital counts
for year, path in file_paths.items():
    data = pd.read_csv(path, encoding='ISO-8859-1')
    
    short_term_hospitals = data[(data['PRVDR_CTGRY_CD'] == 1.0) & (data['PRVDR_CTGRY_SBTYP_CD'] == 1.0)]
    
    unique_count = short_term_hospitals['PRVDR_NUM'].nunique()
    unique_hospitals_by_year[year] = unique_count
    
    # Print unique hospital count per year
    print(f"Year {year}: {unique_count} unique short-term hospitals")

# Convert to Series for plotting
unique_hospitals_series = pd.Series(unique_hospitals_by_year)

# Plotting
plt.figure(figsize=(8, 6))
unique_hospitals_series.plot(kind='bar', color='skyblue')
plt.ylim(6950, 8000)  
plt.xticks(rotation=45)
plt.xlabel('Year')
plt.ylabel('Number of Unique Hospitals')
plt.title('Number of Unique Short-Term Hospitals by Year')
plt.show()
```

b.
By comparison to the outcomes and plot of previous step. I can find out that the plots are fully the same. As the number of unique hospitals (based on short-term hospital) and the number of the short-term hospital are the same across four years.

## Identify hospital closures in POS file (15 pts) (*)

## 1. 

```{python}

import pandas as pd

# Suppress SettingWithCopyWarning
pd.options.mode.chained_assignment = None
# Your existing code
import time
# Define file paths
files = {
    2016: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2016.csv',
    2017: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2017.csv',
    2018: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2018.csv',
    2019: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2019.csv'
}

# Store data for each year
data_by_year = {}

# Load data for each year and filter short-term hospitals (PRVDR_CTGRY_CD and PRVDR_CTGRY_SBTYP_CD both equal to 1)
for year, file in files.items():
    data = pd.read_csv(file, encoding='ISO-8859-1')
    data_filtered = data[(data['PRVDR_CTGRY_CD'] == 1) & (data['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    data_filtered['Year'] = year  # Add year column for easier identification later
    data_by_year[year] = data_filtered[['PRVDR_NUM', 'FAC_NAME', 'CITY_NAME', 'STATE_CD', 'PGM_TRMNTN_CD', 'ZIP_CD', 'Year']]

# Combine all years into a single DataFrame
df_combine = pd.concat(data_by_year.values())

# Get active hospitals in 2016
active_2016 = df_combine[(df_combine['Year'] == 2016) & (df_combine['PGM_TRMNTN_CD'] == 0)]

# Initialize a list to store suspected closures
suspected_closures_data = []

# Check for closures by iterating from 2017 to 2019
for year in range(2017, 2020):  # Starting from 2017
    current_year_data = df_combine[df_combine['Year'] == year]
    active_this_year = set(current_year_data[current_year_data['PGM_TRMNTN_CD'] == 0]['PRVDR_NUM'])

    for index, hospital in active_2016.iterrows():
        if hospital['PRVDR_NUM'] not in active_this_year:
            # Get the termination code for the hospital if it exists in the closure year dataset
            termination_code = 'Unknown'
            matching_hospitals = current_year_data[current_year_data['PRVDR_NUM'] == hospital['PRVDR_NUM']]
            if not matching_hospitals.empty:
                termination_code = matching_hospitals['PGM_TRMNTN_CD'].iloc[0]

            # Store data as a dictionary and append to the list
            suspected_closures_data.append({
                'PRVDR_NUM': hospital['PRVDR_NUM'],
                'FAC_NAME': hospital['FAC_NAME'],
                'ZIP_CD': hospital['ZIP_CD'],
                'Year of Closure': year,
                'Termination Code': termination_code
            })

# Create a DataFrame from the list of dictionaries
suspected_closures = pd.DataFrame(suspected_closures_data)

# Remove duplicate entries in case a hospital is recorded more than once
predicted_closures = suspected_closures.drop_duplicates(subset='PRVDR_NUM')

# Display the results
print(f"Total suspected closures: {len(predicted_closures)}")


```

## 2. 
```{python}
# Sort the predicted closures by hospital name (FAC_NAME) and select the first 10 rows
sorted_closures = predicted_closures.sort_values(by='FAC_NAME').head(10)

# Display the names and year of suspected closure for the first 10 rows
print(sorted_closures[['FAC_NAME', 'Year of Closure']])

```


## 3. 

### a.

```{python}
# Initialize a list to store suspected CMS certification numbers for potential mergers
potential_mergers = []

# Iterate through each row in the predicted_closures DataFrame
for index, row in predicted_closures.iterrows():
    # Get the ZIP code and the suspected closure year for the current hospital
    zip_code = row['ZIP_CD']
    closure_year = row['Year of Closure']
    
    # Get data for the year after the suspected closure in the same ZIP code
    next_year_data = df_combine[
        (df_combine['Year'] == closure_year + 1) & 
        (df_combine['ZIP_CD'] == zip_code) & 
        (df_combine['PGM_TRMNTN_CD'] == 0)
    ]
    
    # Get data for the year of the suspected closure in the same ZIP code
    current_year_data = df_combine[
        (df_combine['Year'] == closure_year) & 
        (df_combine['ZIP_CD'] == zip_code) & 
        (df_combine['PGM_TRMNTN_CD'] == 0)
    ]
    
    # If the number of active hospitals in the next year is not less than the current year,
    # it suggests a potential merger or acquisition, so add the hospital to potential_mergers
    if len(next_year_data) >= len(current_year_data):
        potential_mergers.append(row['PRVDR_NUM'])


# Display the count of suspected mergers/acquisitions
print("Number of hospitals potentially involved in a merger/acquisition:", len(potential_mergers))
```

### b.
```{python}
# Filter out hospitals identified as potential mergers/acquisitions from predicted_closures
hospital_closures_filtered = predicted_closures[~predicted_closures['PRVDR_NUM'].isin(potential_mergers)]

print(f"Number of hospital closures after excluding mergers/acquisitions: {len(hospital_closures_filtered)}")
```

### c.
```{python}
# Sort the corrected closures by hospital name (FAC_NAME) and select the first 10 rows
sorted_corrected_closures = hospital_closures_filtered.sort_values(by='FAC_NAME').head(10)

# Display the names and year of suspected closure for the first 10 rows
print(sorted_corrected_closures[['FAC_NAME', 'Year of Closure']])

```

## Download Census zip code shapefile (10 pt) 

1. 
a. 
The five files are DBF file, PRJ file, SHP file, SHX file and xmlfile. The DBF file is a database file attribute data associated with each ZCTA, including fields for unique ZCTA ID, name, area, and other relevant information.The SHX file is the index file, which facilitates indexing between the .dbf and .shp files, enabling efficient access to spatial and attribute data. The PRj file is the projection file, containing coordinate system and projection details. This file specifies that the data uses the North American Datum of 1983 (NAD83) in decimal degrees. The xml file is the metadata file describing the contents, purpose, usage restrictions, and accuracy of the dataset. The SHP file is a key component of the shapefile format, which Contains the actual shapes (geometry) of features.

b. 
After unzipping, the DBF file is 6,275 KB. The RPJ file is 1 KB. The SHP file is 817,915 KB. The SHX file is 259 KB. The xml file is 16 KB.

2. 
```{python}
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt

# Define file paths
shapefile_path = "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
pos2016_path = "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2016.csv"

# Load the shapefile
gdf = gpd.read_file(shapefile_path)

# Filter to only Texas zip codes (ZCTA5 that starts with 75, 76, 77, 78, or 79)
gdf['ZCTA5'] = gdf['ZCTA5'].astype(str)
tx_gdf = gdf[gdf['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]

# Load the POS 2016 hospital data
pos_2016_df = pd.read_csv(pos2016_path, encoding='ISO-8859-1')

# Filter short-term hospitals (category and subtype codes are both 1)
short_term_hospitals_2016 = pos_2016_df[(pos_2016_df['PRVDR_CTGRY_CD'] == 1.0) & (pos_2016_df['PRVDR_CTGRY_SBTYP_CD'] == 1.0)]

# Group by zip code and count the number of hospitals per zip code
hospital_counts = short_term_hospitals_2016.groupby('ZIP_CD').size().reset_index(name='hospital_count')

# Convert ZIP_CD to string and add leading zeros to match shapefile format
hospital_counts['ZIP_CD'] = hospital_counts['ZIP_CD'].apply(lambda x: str(int(x)).zfill(5))

# Merge hospital counts with the Texas GeoDataFrame
tx_gdf = tx_gdf.merge(hospital_counts, left_on='ZCTA5', right_on='ZIP_CD', how='left')

# Fill N/A values in 'hospital_count' with 0
tx_gdf['hospital_count'] = tx_gdf['hospital_count'].fillna(0)

# Plot a choropleth map of the number of hospitals per zip code
fig, ax = plt.subplots(1, 1, figsize=(12, 12))
tx_gdf.plot(column='hospital_count', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title('Number of Hospitals by Zip Code in Texas (2016)')
ax.set_axis_off()

# Show the plot
plt.show()
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

## 1. 
```{python}
import geopandas as gpd

# Define the file path for the ZIP code shapefile
shapefile_path = "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"

# Load the shapefile into a GeoDataFrame
zips_all_centroids = gpd.read_file(shapefile_path)

# Calculate the centroid of each ZIP code area
zips_all_centroids['geometry'] = zips_all_centroids['geometry'].centroid

# Display the dimensions and column names of the resulting GeoDataFrame
print("Dimensions of GeoDataFrame:", zips_all_centroids.shape)
print("Columns in GeoDataFrame:", zips_all_centroids.columns)

# Show the first few rows to understand the data structure
print(zips_all_centroids.head())

```

1. **GEO_ID**: This is a unique identifier for each geographic unit, corresponding to a standardized code that includes the country and ZIP code area.

2. **ZCTA5**: This stands for "ZIP Code Tabulation Area (ZCTA)". ZCTAs are generalized representations of ZIP codes used by the U.S. Census Bureau. 

3. **NAME**: This column appears to repeat the ZCTA5 code, so it simply be a label for each ZIP code area, duplicating the ZIP code for easy reference.

4. **LSAD**: "Legal/Statistical Area Description". "ZCTA5" in this column indicates that the area is a 5-digit ZIP Code Tabulation Area.

5. **CENSUSAREA**: This column provides the area of the ZIP Code Tabulation Area in square miles or square kilometers, depending on the dataset's unit. 

6. **geometry**: This column contains geometric data that defines the centroid of each ZIP code area. Each entry in this column is a "POINT" with coordinates (longitude, latitude), representing the central location of the ZIP code area.


## 2. 
```{python}

import geopandas as gpd

# Load the shapefile for all ZIP codes
zips_all_centroids = gpd.read_file("/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp")

# Define ZIP code prefixes for Texas (two-digit)
texas_prefixes = ('75', '76', '77', '78', '79')

# Define ZIP code prefixes for bordering states (could be two or three digits)
bordering_states_prefixes = [
    '73', '74',       # 73-74 
    '870', '871', '872', '873', '874', '875', '876', '877', '878', '879',
'880', '881', '882', '883', '884',  # 870-884
    '700', '701', '702', '703', '704', '705', '706', '707', '708', '709',
'710', '711', '712', '713', '714', '715', '716', '717', '718', '719',
'720', '721', '722', '723', '724', '725', '726', '727', '728', '729'
'715', '716', '717', '718', '719',  # 700-715 & 716-729
]

# Function to check if a ZIP code matches any of the prefixes
def matches_prefix(zip_code, prefixes):
    return any(zip_code.startswith(prefix) for prefix in prefixes)

# Filter ZIP codes for Texas
zips_texas_centroids = zips_all_centroids[zips_all_centroids['ZCTA5'].apply(lambda x: matches_prefix(x, texas_prefixes))]

# Filter ZIP codes for Texas and bordering states
zips_texas_borderstates_centroids = zips_all_centroids[
    zips_all_centroids['ZCTA5'].apply(lambda x: matches_prefix(x, texas_prefixes + tuple(bordering_states_prefixes)))
]

# Get the number of unique ZIP codes in each subset
unique_texas_zips = zips_texas_centroids['ZCTA5'].nunique()
unique_borderstate_zips = zips_texas_borderstates_centroids['ZCTA5'].nunique()

print(f"Number of unique ZIP codes in Texas: {unique_texas_zips}")
print(f"Number of unique ZIP codes in Texas and bordering states: {unique_borderstate_zips}")

```

## 3. 

```{python}

# Load the 2016 hospital data
hospital_data_2016 = pd.read_csv('/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2016.csv', encoding='ISO-8859-1')
# Filter hospitals to only include those with PRVDR_CTGRY_CD and PRVDR_CTGRY_SBTYP_CD equal to 1
hospital_data_2016_filtered = hospital_data_2016[
    (hospital_data_2016['PRVDR_CTGRY_CD'] == 1) & 
    (hospital_data_2016['PRVDR_CTGRY_SBTYP_CD'] == 1)
]

# Clean up ZIP_CD column in hospital data
hospital_data_2016_filtered['ZIP_CD'] = hospital_data_2016_filtered['ZIP_CD'].apply(lambda x: str(int(float(x))))

# Perform an inner merge to find ZIP codes in Texas and bordering states that contain at least one hospital in 2016
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    hospital_data_2016_filtered[['ZIP_CD']].drop_duplicates(),
    left_on='ZCTA5',  # from zip code data
    right_on='ZIP_CD',  # from hospital data
    how='inner'
)

# Output the number of unique ZIP codes with at least one hospital in 2016
unique_zip_count = zips_withhospital_centroids['ZCTA5'].nunique()
print(f"Number of unique ZIP codes with at least one hospital in 2016: {unique_zip_count}")
```

For merge type, it's needed to use an **inner merge** because we are only interested in ZIP codes that appear in both `zips_texas_borderstates_centroids` and `hospital_data_2016_filtered`. 
This ensures that the ZIP codes included in the result contain at least one hospital.

The merge is done on the **ZIP code** variable, specifically using the **`ZCTA5`** column from `zips_texas_borderstates_centroids` and the **`ZIP_CD`** column from `hospital_data_2016_filtered`.

## 4. 

### a.

```{python}
import time
from scipy.spatial import KDTree
import pandas as pd

# Project both datasets to a projected CRS for accurate centroid calculations
projected_texas_centroids = zips_texas_centroids.to_crs(epsg=3857)
projected_hospital_centroids = zips_withhospital_centroids.to_crs(epsg=3857)

# Select a random sample of 10 ZIP codes from the projected Texas ZIP centroids
sample_zips_texas = projected_texas_centroids.sample(n=10, random_state=42)  # Set a random state for reproducibility

# Start timing
start_time_sample = time.time()

# Calculate centroids for the sample of Texas ZIP code polygons
sample_texas_centroids = sample_zips_texas['geometry'].centroid
sample_texas_coords = [(point.x, point.y) for point in sample_texas_centroids]

# Calculate centroids for hospital ZIP code polygons
hospital_centroids = projected_hospital_centroids['geometry'].centroid
hospital_coords = [(point.x, point.y) for point in hospital_centroids]

# Build a KDTree for hospital coordinates to enable efficient nearest-neighbor queries
hospital_tree = KDTree(hospital_coords)

# Query the nearest hospital for each ZIP code in the sample in one batch
sample_distances, sample_indices = hospital_tree.query(sample_texas_coords)

# Create a DataFrame with ZIP codes and their corresponding nearest hospital distance
results_sample = pd.DataFrame({
    "ZIP Code": sample_zips_texas['ZCTA5'],  # Adjust 'ZCTA5' to match the actual ZIP code column name
    "Distance to Nearest Hospital": sample_distances
})

# End timing
end_time_sample = time.time()
elapsed_time_sample = end_time_sample - start_time_sample

# Estimate the total time for all ZIP codes in zips_texas_centroids
num_total_zips = len(zips_texas_centroids)
estimated_total_time = (elapsed_time_sample / 10) * num_total_zips

# Print results
print(f"Time taken for 10 sample ZIP codes: {elapsed_time_sample:.2f} seconds")
print(f"Estimated total time for all ZIP codes: {estimated_total_time:.2f} seconds")
print(results_sample.head())  # Display first few rows of the sample results

```

### b.


```{python}
import time
from scipy.spatial import KDTree
import pandas as pd

# Set control variable
run_full_calculation = True  # Set to True to run the full calculation

# If set to True, perform the full calculation
if run_full_calculation:
    # Start timing for the full calculation
    start_time_full = time.time()

    # Re-project Texas ZIP code polygons and hospital ZIP code polygons to EPSG:3857 (meters)
    projected_texas_centroids = zips_texas_centroids.to_crs(epsg=3857)
    projected_hospital_centroids = zips_withhospital_centroids.to_crs(epsg=3857)

    # Calculate centroids for Texas ZIP code polygons
    texas_centroids = projected_texas_centroids['geometry'].centroid
    texas_coords = [(point.x, point.y) for point in texas_centroids]

    # Calculate centroids for hospital ZIP code polygons
    hospital_centroids = projected_hospital_centroids['geometry'].centroid
    hospital_coords = [(point.x, point.y) for point in hospital_centroids]

    # Build a KDTree for hospital coordinates to enable efficient nearest-neighbor queries
    hospital_tree = KDTree(hospital_coords)

    # Query the nearest hospital for each Texas ZIP code in one batch
    distances, indices = hospital_tree.query(texas_coords)

    # Create a DataFrame with ZIP codes and their corresponding nearest hospital distance
    results_full = pd.DataFrame({
        "ZIP Code": projected_texas_centroids['ZCTA5'],  # Adjust 'ZCTA5' to match the actual ZIP code column name
        "Distance to Nearest Hospital": distances
    })

    # End timing for the full calculation
    end_time_full = time.time()
    elapsed_time_full = end_time_full - start_time_full

    # Print results
    print(f"Time taken for the full calculation: {elapsed_time_full:.2f} seconds")
    print(results_full.head())  # Display first few rows of the results

else:
    # Skip the full calculation part
    print("Skipping full calculation in preview mode.")

```

The time it took to run the entire dataset was about 0.65 seconds, which is actually much faster than the estimated 5 seconds, most likely because it takes time to open and run the program itself, and when the program is running it takes time very quickly, so the estimated time will be quite different from the actual time

### c.

Units: EPSG:3857 (meters), we need to change the meters into miles.
```{python}

METERS_TO_MILES = 0.00062  # This is an estimation from meters to miles

# Convert distances from degrees to miles and add as a new column in the DataFrame
results_full['Distance_Miles'] = results_full['Distance to Nearest Hospital'] * METERS_TO_MILES

```

## 5. 

### a.
Right now, the distance is based on the Column of Distance_Miles, so the unit is miles.

### b.
```{python}
# Calculate the average distance in miles
average_distance_miles = sum(results_full['Distance_Miles']) / len(results_full['Distance_Miles'])

# Display the average distance
print(f"Average distance to the nearest hospital: {average_distance_miles:.2f} miles")

# Evaluate if the value makes sense
if average_distance_miles < 5:
    print("This average distance seems quite low, suggesting that most ZIP codes are close to a hospital.")
elif average_distance_miles < 15:
    print("This average distance is reasonable, indicating that hospitals are within moderate proximity for most ZIP codes.")
else:
    print("This average distance is quite high, which might suggest limited hospital access in some areas.")

```

### c.

```{python}

import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import geopandas as gpd

# Ensure 'ZIP Code' column is of string type for compatibility with spatial joins
results_full['ZIP Code'] = results_full['ZIP Code'].astype(str)

# Drop 'ZIP Code' and 'Distance_Miles' columns if they already exist in zips_texas_centroids to avoid conflicts
if 'ZIP Code' in zips_texas_centroids.columns:
    zips_texas_centroids = zips_texas_centroids.drop(columns=['ZIP Code'])
if 'Distance_Miles' in zips_texas_centroids.columns:
    zips_texas_centroids = zips_texas_centroids.drop(columns=['Distance_Miles'])

# Perform the merge
zips_texas_centroids = zips_texas_centroids.merge(
    results_full[['ZIP Code', 'Distance_Miles']], 
    left_on='ZCTA5', right_on='ZIP Code', how='left'
)

# Print columns to verify 'Distance_Miles' is added
print("Columns in zips_texas_centroids after merge:", zips_texas_centroids.columns)

# Check if 'Distance_Miles' exists before plotting
if 'Distance_Miles' in zips_texas_centroids.columns:
    # Define a red color map with adjusted normalization for more contrast in lighter areas
    vmin, vmax = 0, 60  # Adjust the maximum value to make the colors darker overall
    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)

    # Plot the Texas ZIP codes and color by 'Distance_Miles'
    fig, ax = plt.subplots(1, 1, figsize=(6, 6))
    zips_texas_centroids.plot(column='Distance_Miles', 
                              cmap='Reds',  # Use a red color map
                              legend=True, 
                              ax=ax,
                              legend_kwds={'label': "Distance to Nearest Hospital (Miles)", 'orientation': "horizontal"},
                              norm=norm)
    ax.set_title("Average Distance to Nearest Hospital for ZIP Codes in Texas")
    ax.set_axis_off()

    plt.show()
else:
    print("Error: 'Distance_Miles' column not found in zips_texas_centroids.")

```


## Effects of closures on access in Texas (15 pts)

1. 
```{python}
import pandas as pd

# list file paths
files = {
    2016: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2016.csv',
    2017: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2017.csv',
    2018: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2018.csv',
    2019: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2019.csv'
}

# Retore data by year
data_by_year = {}

# Upload data by year and filter short-term hospitals
for year, file in files.items():
    data = pd.read_csv(file, encoding='ISO-8859-1')
    data_filtered = data[(data['PRVDR_CTGRY_CD'] == 1) & (data['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    data_filtered['Year'] = year  
    data_by_year[year] = data_filtered[['PRVDR_NUM', 'FAC_NAME', 'CITY_NAME', 'STATE_CD', 'PGM_TRMNTN_CD', 'ZIP_CD', 'Year']]

# combine data
df_combine = pd.concat(data_by_year.values())

# Filter all closed hopitals
tx_closures = df_combine[(df_combine['STATE_CD'] == 'TX') & (df_combine['PGM_TRMNTN_CD'] != 0)]

# Conclude closure times by zid code
zip_code_closures = tx_closures['ZIP_CD'].value_counts().reset_index()
zip_code_closures.columns = ['ZIP_CD', 'closure_count']

# Indicate the outcomes
closure_summary = zip_code_closures['closure_count'].value_counts().reset_index()
closure_summary.columns = ['closure_count', 'num_zip_codes']

# Print the outcomes
print("Directly affected Texas ZIP codes with closure count:")
print(zip_code_closures)
print("\nNumber of zip codes vs. number of closures they experienced:")
print(closure_summary)
```

2. 
```{python}
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt

# File path for SHP file
shapefile_path = "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
gdf = gpd.read_file(shapefile_path)

# Convert ZCTA5 to string
gdf['ZCTA5'] = gdf['ZCTA5'].astype(str)

# Filter Texas zip codes
tx_gdf = gdf[gdf['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]

# Apply zip_code_closures to filter zip codes with at least one closure
affected_zip_codes = zip_code_closures[zip_code_closures['closure_count'] > 0]
affected_zip_codes['ZIP_CD'] = affected_zip_codes['ZIP_CD'].apply(lambda x: str(int(float(x))))

# Merge data
merged_gdf = tx_gdf.merge(affected_zip_codes, left_on='ZCTA5', right_on='ZIP_CD', how='inner')

# Filter unnecessary geographic data
merged_gdf = merged_gdf[merged_gdf.geometry.notnull() & merged_gdf.is_valid]

# Draw the map with color shades based on closure count
fig, ax = plt.subplots(1, 1, figsize=(10, 10))
merged_gdf.plot(column='closure_count', cmap='OrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)
ax.set_title("Texas Zip Codes Affected by Hospital Closures (2016-2019)")
ax.set_axis_off()

# Show the plot
plt.show()

# Print the number of directly affected zip codes
affected_zip_count = merged_gdf['ZCTA5'].nunique()
print(f"There are {affected_zip_count} directly affected zip codes in Texas.")
```

3. 
```{python}
import geopandas as gpd
import pandas as pd

# Convert ZIP codes and ensure consistent format
directly_affected_zip_codes = zip_code_closures[zip_code_closures['closure_count'] > 0]
directly_affected_zip_codes['ZIP_CD'] = directly_affected_zip_codes['ZIP_CD'].apply(lambda x: str(int(float(x))))

# Create the GeoDataFrame of directly affected ZIP codes
directly_affected_gdf = tx_gdf[tx_gdf['ZCTA5'].isin(directly_affected_zip_codes['ZIP_CD'])].copy()

# Create a 10-mile buffer around directly affected ZIP codes
directly_affected_gdf['geometry'] = directly_affected_gdf.geometry.buffer(10 * 1609.34)  

# Perform a spatial join to find ZIP codes within the buffer zone
indirectly_affected_gdf = gpd.sjoin(tx_gdf, directly_affected_gdf, how="inner", predicate="intersects")

# Remove directly affected ZIP codes to keep only those indirectly affected
indirectly_affected_zip_codes = indirectly_affected_gdf[~indirectly_affected_gdf['ZCTA5_left'].isin(directly_affected_gdf['ZCTA5'])]

# Calculate the number of indirectly affected ZIP codes
indirectly_affected_zip_count = indirectly_affected_zip_codes['ZCTA5_left'].nunique()
print(f"There are {indirectly_affected_zip_count} indirectly affected ZIP codes in Texas.")
```

4. 
```{python}
import geopandas as gpd
import pandas as pd
import matplotlib.pyplot as plt

# File paths for the shapefile and hospital closure data
shapefile_path = "/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
files = {
    2016: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2016.csv',
    2017: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2017.csv',
    2018: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2018.csv',
    2019: '/Users/xiadizhe/Documents/GitHub/problem-set-4-kevin-dylan/pos2019.csv'
}

# Load shapefile and filter for Texas ZIP codes
gdf = gpd.read_file(shapefile_path)
gdf['ZCTA5'] = gdf['ZCTA5'].astype(str)
tx_gdf = gdf[gdf['ZCTA5'].str.startswith(('75', '76', '77', '78', '79'))]

# Set a common CRS for accurate distance calculations, e.g., UTM zone for Texas
tx_gdf = tx_gdf.to_crs(epsg=32614)

# Load and combine closure data for each year, filtering for short-term hospitals
data_by_year = {}
for year, file in files.items():
    data = pd.read_csv(file, encoding='ISO-8859-1', low_memory=False)
    data_filtered = data[(data['PRVDR_CTGRY_CD'] == 1) & (data['PRVDR_CTGRY_SBTYP_CD'] == 1)]
    data_filtered.loc[:, 'Year'] = year
    data_by_year[year] = data_filtered[['PRVDR_NUM', 'FAC_NAME', 'CITY_NAME', 'STATE_CD', 'PGM_TRMNTN_CD', 'ZIP_CD', 'Year']]

# Combine data and filter for Texas closures
df_combine = pd.concat(data_by_year.values())
tx_closures = df_combine[(df_combine['STATE_CD'] == 'TX') & (df_combine['PGM_TRMNTN_CD'] != 0)]

# Count closures by ZIP code
zip_code_closures = tx_closures['ZIP_CD'].value_counts().reset_index()
zip_code_closures.columns = ['ZIP_CD', 'closure_count']

# Identify directly affected ZIP codes
directly_affected_zip_codes = zip_code_closures[zip_code_closures['closure_count'] > 0]
directly_affected_zip_codes['ZIP_CD'] = directly_affected_zip_codes['ZIP_CD'].apply(lambda x: str(int(float(x))))

# Create GeoDataFrame of directly affected ZIP codes
directly_affected_gdf = tx_gdf[tx_gdf['ZCTA5'].isin(directly_affected_zip_codes['ZIP_CD'])].copy()
directly_affected_gdf['category'] = 'Directly Affected'

# Create a 10-mile buffer around directly affected ZIP codes
directly_affected_gdf['geometry'] = directly_affected_gdf.geometry.buffer(10 * 1609.34)  # 10 miles in meters

# Perform a spatial join to find indirectly affected ZIP codes within the buffer
indirectly_affected_gdf = gpd.sjoin(tx_gdf, directly_affected_gdf, how="inner", predicate="intersects")
indirectly_affected_gdf = indirectly_affected_gdf[~indirectly_affected_gdf['ZCTA5_left'].isin(directly_affected_gdf['ZCTA5'])]
indirectly_affected_gdf['category'] = 'Indirectly Affected'

# Create a GeoDataFrame for unaffected ZIP codes
not_affected_gdf = tx_gdf[~tx_gdf['ZCTA5'].isin(directly_affected_zip_codes['ZIP_CD']) & 
                          ~tx_gdf['ZCTA5'].isin(indirectly_affected_gdf['ZCTA5_left'])]
not_affected_gdf['category'] = 'Not Affected'

# Combine all three categories into a single GeoDataFrame
final_gdf = pd.concat([
    directly_affected_gdf[['ZCTA5', 'geometry', 'category']],
    indirectly_affected_gdf[['ZCTA5_left', 'geometry', 'category']].rename(columns={'ZCTA5_left': 'ZCTA5'}),
    not_affected_gdf[['ZCTA5', 'geometry', 'category']]
])

# Ensure the final_gdf is also in the same CRS for consistency
final_gdf = final_gdf.to_crs(epsg=32614)

# Plot the choropleth map with different colors for each category
fig, ax = plt.subplots(1, 1, figsize=(12, 12))
final_gdf.plot(column='category', cmap='Set1', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True, categorical=True)

# Set title and remove axis
ax.set_title("Texas ZIP Codes Affected by Hospital Closures (2016-2019)")
ax.set_axis_off()

# Show plot
plt.show()
```


## Reflecting on the exercise (10 pts) 

### 1. Partner 1:

There would be misunderstanding of temporary closures and permanent closures. Some hospitals close temporarily for various reasons and then reopen. A simple method may mistake these for permanent closures. In addition, there would be misunderstanding of changes in ownership or merger. Hospitals that merge or change ownership might be listed as different names in the dataset. The "first-pass" approach could mistakenly classify these cases as closures. 

In order to avoid these potential problems, we can do multi-year analysis. We can make sure that whether a hospital is closing or not by look at how it has been operated over several years. If it has been closed for a few years, it is probably permanent closing. Meanwhile, we can also analyze from cross-sectional databases. We can check other public records for more accurate data on hospital openings and closures. Such as, procurement data from major suppliers and tax payment record.

### 2. Partner 2:

We identify zip codes affected by hospital closures by observing the termination status of hospitals within each zip code between different years. This approach provides a straightforward way to detect potential gaps in healthcare accessibility caused by closures. However, it only captures the presence or absence of hospitals without accounting for the proximity of hospitals outside the zip code boundaries. 

Besides, it is reasonable to worry about our assumption that any zip code with a hospital closure will experiences a reduction in hospital resource immediately. But in real world, especially in urban areas, multiple hospital oftern serve overlapping regions.

Improvements:
1. Take Population Density and Demographics into weight calculation process; 2. Use Travel Time Instead of Distance, which will focus on convinience rather than pure distance; 3. Analyze Hospital Capacity Changes
